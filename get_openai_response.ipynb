{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from help_funcs import *\n",
    "import openai\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Response': 'Привет!\\n\\nМеня зовут Бубльгум, я помощник руководителя компании.\\n\\nСейчас мы проводим небольшой опрос, чтобы понять, насколько вы удовлетворены работой нашей компании. Пожалуйста, ответьте на несколько вопросов, это займет не более 5 минут.\\n\\nСпасибо! Ваше мнение очень важно для нас.',\n",
       " 'Eval': None,\n",
       " 'Comment': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructions['43']['Yandex_GPT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = make_open_ai_request(prompt=make_prompt(key='1', model='Yandex_GPT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TRUE.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.json()['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TRUE', '')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_eval(x.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "Unable to extract eval and comment\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n"
     ]
    }
   ],
   "source": [
    "force = False\n",
    "for k in instructions.keys():\n",
    "    if instructions[k]['Gigachat']['Eval'] != None and instructions[k]['Yandex_GPT']['Eval'] != None:\n",
    "        if force == False:\n",
    "            print(k)\n",
    "            continue\n",
    "    print(k)\n",
    "    ya_prompt = make_prompt(key=k, model='Yandex_GPT')\n",
    "    giga_prompt = make_prompt(key=k, model='Gigachat')\n",
    "    ya_response = make_open_ai_request(ya_prompt)\n",
    "    try: \n",
    "        evaluation, comment = get_eval(ya_response.json())\n",
    "    except:\n",
    "        print('Unable to extract eval and comment')\n",
    "        continue\n",
    "    instructions[k]['Yandex_GPT']['Comment'] = comment\n",
    "    instructions[k]['Yandex_GPT']['Eval'] = evaluation\n",
    "    time.sleep(5)\n",
    "    giga_response = make_open_ai_request(giga_prompt)\n",
    "    try:\n",
    "        evaluation, comment = get_eval(giga_response.json())\n",
    "    except:\n",
    "        print('Unable to extract eval and comment')\n",
    "        continue\n",
    "    instructions[k]['Gigachat']['Comment'] = comment\n",
    "    instructions[k]['Gigachat']['Eval'] = evaluation\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('instructions.json', 'w', encoding='utf-8') as w:\n",
    "    w.write(json.dumps(instructions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = make_open_ai_request(make_prompt('166', 'Yandex_GPT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1864"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = make_prompt('166', 'Yandex_GPT')[521:]\n",
    "len(pr.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "886"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(instructions['166']['Yandex_GPT']['Response'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import post\n",
    "def make_open_ai_request(prompt=None, token=OPENAI_API_KEY):\n",
    "    if prompt == None:\n",
    "        raise Exception('Provide prompt')\n",
    "    url = 'https://api.openai.com/v1/chat/completions'\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {token}\"\n",
    "    }\n",
    "    body = {\n",
    "        'model' : 'gpt-4', #changed it here because it accepts a longer input.\n",
    "        'messages' : [{'role' : 'user', 'content' : prompt}],\n",
    "        'temperature' : 0.7\n",
    "    }\n",
    "    return post(url, headers=headers, json=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = make_open_ai_request(make_prompt('166', 'Gigachat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-8GyXbZHHnxxzR6fqWZcS24YCgZ75J',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1699056659,\n",
       " 'model': 'gpt-4-0613',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant', 'content': 'TRUE'},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 1828,\n",
       "  'completion_tokens': 1,\n",
       "  'total_tokens': 1829}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Сколько байт занимает целое число типа int в памяти компьютера?',\n",
       " 'type': 'теория программирования',\n",
       " 'Yandex_GPT': {'Response': '4 байта', 'Eval': 'TRUE', 'Comment': ''},\n",
       " 'Gigachat': {'Response': 'Целое число типа int занимает 4 байта в памяти компьютера.',\n",
       "  'Eval': 'TRUE',\n",
       "  'Comment': ''}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructions['167']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Запиши самое важное из этой статьи на русском языке в 5-10 предложениях:\\n \\n\\n Llama 2\\n Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model. Links to other models can be found in the index at the bottom.\\n \\n\\n Model Details\\n Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.\\n Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.\\n Model Developers Meta\\n Variations Llama 2 comes in a range of parameter sizes — 7B, 13B, and 70B — as well as pretrained and fine-tuned variations.\\n Input Models input text only.\\n Output Models generate text only.\\n Model Architecture Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.\\n \\n\\n Training Data\\n Params\\n Content Length\\n GQA\\n Tokens\\n LR\\n Llama 2\\n A new mix of publicly available online data\\n 7B\\n 4k\\n ✗\\n 2.0T\\n 3.0 x 10-4\\n Llama 2\\n A new mix of publicly available online data\\n 13B\\n 4k\\n ✗\\n 2.0T\\n 3.0 x 10-4\\n Llama 2\\n A new mix of publicly available online data\\n 70B\\n 4k\\n ✔\\n 2.0T\\n 1.5 x 10-4\\n Llama 2 family of models. Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models - 70B -- use Grouped-Query Attention (GQA) for improved inference scalability.\\n Model Dates Llama 2 was trained between January 2023 and July 2023.\\n Status This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\\n Research Paper \"Llama-2: Open Foundation and Fine-tuned Chat Models\"\\n \\n\\n Intended Use\\n Intended Use Cases Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\\n To get the expected features and performance for the chat versions, a specific formatting needs to be followed, including the INST and <<SYS>> tags, BOS and EOS tokens, and the whitespaces and breaklines in between (we recommend calling strip() on inputs to avoid double-spaces). See our reference code in github for details: chat_completion.\\n Out-of-scope Uses Use in any manner that violates applicable laws or regulations (including trade compliance laws).Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2.\\n \\n\\n Hardware and Software\\n Training Factors We used custom training libraries, Meta\\'s Research Super Cluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\\n Carbon Footprint Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539 tCO2eq, 100% of which were offset by Meta’s sustainability program.\\n CO2 emissions during pretraining. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta\\'s sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\\n \\n\\n Training Data\\n Overview Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\\n Overall performance on grouped academic benchmarks. Code: We report the average pass@1 scores of our models on HumanEval and MBPP. Commonsense Reasoning: We report the average of PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA, and CommonsenseQA. We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. World Knowledge: We evaluate the 5-shot performance on NaturalQuestions and TriviaQA and report the average. Reading Comprehension: For reading comprehension, we report the 0-shot average on SQuAD, QuAC, and BoolQ. MATH: We report the average of the GSM8K (8 shot) and MATH (4 shot) benchmarks at top 1.\\n Evaluation of pretrained LLMs on automatic safety benchmarks. For TruthfulQA, we present the percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we present the percentage of toxic generations (the smaller the better).\\n Evaluation of fine-tuned LLMs on different safety datasets. Same metric definitions as above.\\n \\n\\n Ethical Considerations and Limitations\\n Llama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.\\n Please see the Responsible Use Guide available at https://ai.meta.com/llama/responsible-use-guide/\\n \\n\\n Reporting Issues\\n Please report any software “bug,” or other problems with the models through one of the following means:\\n  • Reporting issues with the model: github.com/facebookresearch/llama\\n  • Reporting problematic content generated by the model: developers.facebook.com/llama_output_feedback\\n  • Reporting bugs and security concerns: facebook.com/whitehat/info',\n",
       " 'type': 'суммаризация',\n",
       " 'Yandex_GPT': {'Response': 'Llama 2 - это набор предварительно обученных и дообученных моделей генерации текста с размером от 7 миллиардов до 70 миллиардов параметров. Это репозиторий для предварительно обученной модели размером 7 миллиардов. Ссылки на другие модели можно найти в индексе внизу.\\n\\nМодель Details\\nИспользование этой модели регулируется лицензией Meta. Для загрузки весов модели и токенизатора, пожалуйста, посетите веб-сайт и примите нашу лицензию перед запросом доступа здесь.\\nMeta разработала и публично выпустила семейство больших языковых моделей LLM (LLMs) Llama 2, коллекцию предварительно обученных и дообученных текстовых моделей генерации с размером параметров от 7 миллиардов до 70 миллиардов. Наши дообученные модели LLM, называемые Llama-2-Chat, оптимизированы для диалоговых сценариев использования. Llama-2-Chat модели превосходят общедоступные модели чата в большинстве тестов, и в наших оценках для полезности и безопасности, они сопоставимы с некоторыми популярными закрытыми моделями, такими как ChatGPT и PaLM.\\nРазработчики Meta\\nМодели Llama 2 поставляются с различными размерами параметров - 7B, 13B и 70B - а также предварительно обученными и дообученными версиями.\\nВход Модели принимают только текст.\\nВыходные Модели генерируют только текст.\\nМодель Архитектура Llama 2 использует оптимизированную архитектуру трансформатора. Откалиброванные версии используют контролируемое обучение с сужением (SFT) и обучение с подкреплением с человеческими отзывами (RLHF), чтобы согласовать их с предпочтениями людей в отношении полезности и безопасности.\\nОбучение Данные\\nПараметры\\nДлина Контента\\nGQA\\nТокены\\nLR\\nLlama 2\\nНовый набор публично доступных онлайн данных\\n7B\\n4к\\n✗\\n2.0Т\\n3.0 х 10-4\\nLlama 2\\nНовый набор публично доступных онлайн данных\\n13B\\n4к\\n✗\\n2.0Т\\n3.0 х 10-4\\nLlama 2\\nНовый набор публично доступных онлайн данных\\n70B\\n4к\\n✔\\n2.0Т\\n1.5 х 10-4\\nСемейство моделей Llama 2. Точное количество токенов относится к обучающим данным только. Все модели обучаются с глобальным размером пакета 4 миллиона токенов. Более крупные модели - 70B - используют групповую обработку запросов (GQA) для улучшения масштабируемости вывода.\\nДаты обучения Llama 2 была обучена в период с января 2023 по июль 2023 года.\\nСостояние Это статическая модель, обученная на офлайн-данных. В будущем мы выпустим откалиброванные модели по мере улучшения безопасности модели с отзывами сообщества.\\nИсследовательская статья \"Llama-2: Открытые основы и доработанные чат-модели\"\\nПредназначенное использование\\nLlama 2 предназначен для коммерческого и исследовательского использования на английском языке. Откалиброванные модели предназначены для похожего на ассистента чата, тогда как предварительно обученные модели могут быть адаптированы для различных задач генерации естественного языка.\\nДля достижения ожидаемых характеристик и производительности в чат-версиях необходимо следовать специфическому формату, включая теги INST и <<SYS>>, маркеры BOS и EOS, а также пробелы и разрывы между ними (мы рекомендуем вызывать strip() для входных данных, чтобы избежать двойных пробелов). Смотрите наш пример кода в github для подробностей: chat_completion.\\nЗа пределами сферы использования Использование любым способом, который нарушает применимые законы и правила (включая комплаенс-законы). Использование на любых языках, кроме английского. Использование любым другим способом, который запрещен Лицензионным соглашением и Политикой использования Llama 2.\\nОборудование и Программное Обеспечение\\nФакторы обучения Мы использовали собственные библиотеки обучения, исследовательскую кластерную систему Meta и кластеры производства для предварительного обучения. Дообучение, аннотирование и оценка также выполнялись на сторонних облачных вычислительных ресурсах.\\nВыбросы углерода Предварительное обучение потребовало 3,3 миллиона часов GPU-времени на оборудовании типа A100-80 ГБ (TDP 350-400 Вт). Предполагаемый общий выброс CO2 составил 539 тонн CO2 эквивалента, 100 % из которых были компенсированы программой устойчивости Meta.\\nВыбросы CO2 во время предварительного обучения. Время: общее время GPU, необходимое для обучения каждой модели. Потребляемая мощность: пиковая мощность устройства GPU, скорректированная с учетом эффективности использования энергии. 100 % выбросов компенсируются программой устойчивости Meta, и поскольку мы открыто выпускаем эти модели, затраты на предварительное обучение не должны нести другие.\\nОбучение Данным\\nОбзор Llama 2 предварительно обучен на 2 триллиона токенов данных из общедоступных источников. Тонкая настройка включает общедоступные наборы инструкций, а также более одного миллиона новых вручную аннотированных примеров. Ни в предварительном обучении, ни в тонкой настройке данные не включают пользовательские данные Meta.\\nОбщая производительность на групповых академических тестах. Код: Мы сообщаем средние проходы @1 баллов наших моделей на HumanEval и MBPP. Логическое мышление: Мы сообщаем среднее значение PIQA, SIQA, HellaSwag, WinoGrande, ARC легкий и вызов, OpenBookQA и CommonsenseQA с 7 выстрелами. Мы проводим оценку 0 выстрелов для всех остальных тестов. Знания мира: Мы оцениваем 5-выстрельную производительность на NaturalQuestions и TriviaQA и сообщаем среднее значение. Чтение: Для чтения мы сообщаем 0-выстрельное среднее значение на SQuAD, QuAC и BoolQ. МАТЕМАТИКА: Мы сообщаем среднее значение GSM8K (8 выстрелов) и MATH (4 выстрела) на вершине 1.\\nОценка предварительно обученных LLM на автоматических тестах безопасности. Для TruthfulQA мы представляем процент поколений, которые правдивы и информативны (чем выше, тем лучше). Для ToxiGen мы представляем процент токсичных поколений (чем меньше, тем лучше).\\nОценка доработанных LLM на различных наборах данных безопасности. Те же определения метрик, что и выше.\\nЭтические соображения и ограничения\\nLlama 2 - новая технология, которая несет риски при использовании. Тестирование до настоящего времени проводилось только на английском языке, и не охватывало и не могло охватить все сценарии. По этим причинам, как и для всех LLM, потенциальные ответы модели не могут быть предсказаны заранее, и модель может в некоторых случаях генерировать неточные, предвзятые или другие нежелательные ответы на пользовательские запросы. Поэтому перед развертыванием любых приложений Llama 2 разработчики должны проводить тестирование безопасности и настройку, адаптированную к их конкретным приложениям модели.\\nПожалуйста, обратитесь к Руководству по ответственному использованию, доступному по адресу https://ai.meta.com/llama/ответственное-использование-руководство/.\\nОтчет об ошибках\\nЕсли есть проблемы с моделями, пожалуйста, сообщите об этом через следующие каналы:\\n• Проблемы с моделью: github.com/ facebookresearch/ llama\\n• Проблемный контент, сгенерированный моделью: developers. facebook. com/llama_ output_feedback\\n• Сообщения о проблемах и безопасности: facebook. com/whitehat /info',\n",
       "  'Eval': None,\n",
       "  'Comment': None},\n",
       " 'Gigachat': {'Response': 'Статья описывает коллекцию моделей Llama 2, которые являются предварительно обученными и дообученными генеративными моделями для обработки текста. Эти модели имеют различные размеры параметров от 7 миллиардов до 70 миллиардов и предназначены для использования в чат-ботах и других приложениях обработки естественного языка. Модели были обучены на большом объеме данных и могут генерировать текст, который может быть полезным и безопасным. Однако, как и любая технология, они не лишены рисков, поэтому разработчики должны проводить тестирование безопасности перед использованием этих моделей в своих приложениях.',\n",
       "  'Eval': None,\n",
       "  'Comment': None}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructions['166']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
